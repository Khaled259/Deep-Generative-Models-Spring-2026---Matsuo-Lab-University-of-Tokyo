***

# Deep Learning Applied Course: Deep Generative Models
**Lecture 1: Overview of Generative Models**
*Based on materials by Matsuo-Iwasawa Lab, The University of Tokyo*

## Table of Contents
1.  [Introduction](#1-introduction)
2.  [What is a Generative Model?](#2-what-is-a-generative-model)
3.  [Learning Generative Models](#3-learning-generative-models)
4.  [Examples of Learning](#4-examples-of-learning)
5.  [Introduction to Latent Variables](#5-introduction-to-latent-variables)
6.  [Appendix: MAP Estimation](#6-appendix-map-estimation)

---

## 1. Introduction

This course focuses on **Deep Generative Models**, a field that has gained massive attention recently as the foundational technology behind "Generative AI" (e.g., image generation, text generation). Beyond just generating content, these models are applied in anomaly detection, semi-supervised learning, representation learning, and meta-learning.

**Course Goal:** To comprehensively learn the theoretical background and foundational knowledge of deep generative models.

**Prerequisites:**
*   Basic Deep Learning knowledge.
*   University-level Linear Algebra, Calculus, and Probability/Statistics.

> **"What I cannot create, I do not understand."** â€” *Richard P. Feynman*
>
> This quote embodies the philosophy of this course (the constructive approach): by building a model that can generate data, we truly understand the data.

---

## 2. What is a Generative Model?

### The Concept
Imagine we have a dataset of face images. We want to understand "what makes a face image?" Since we cannot define every possible pixel combination manually, we imagine a **"Face Image Generator"**.
*   If we press a button on this generator, it produces a random face image.
*   The set of all images generated by this machine constitutes the concept of "Face Images."

### Formal Definition
1.  **Data Distribution ($p_{data}(x)$):**
    We assume the real-world data (e.g., faces) comes from an unknown true distribution $p_{data}(x)$. Since we don't have this distribution, we only have a **dataset** $\{x_1, ..., x_N\}$ sampled from it.

2.  **Model Distribution ($p_{\theta}(x)$):**
    We construct a probabilistic model $p_{\theta}(x)$ parameterized by $\theta$. This is the **Generative Model**.

3.  **Goal:**
    We want to approximate the true distribution with our model:
    $$ p_{\theta}(x) \approx p_{data}(x) $$

### Capabilities of Generative Models
*   **Generation:** Sampling $x \sim p_{\theta}(x)$ creates new, unseen data.
*   **Density Estimation:** Input a data point $x$ to calculate $p_{\theta}(x)$. This tells us how likely a data point is (useful for anomaly detection).
*   **Inpainting/Denoising:** Completing missing parts of data or removing noise.

### Generative vs. Discriminative Models

| Feature | **Discriminative Model** | **Generative Model** |
| :--- | :--- | :--- |
| **Goal** | Predict label $y$ from input $x$. | Learn the distribution of input $x$ (and $y$). |
| **Probability** | Models conditional probability $p(y|x)$. | Models joint probability $p(x, y)$ or just $p(x)$. |
| **Focus** | Finds the decision boundary between classes. | Models the underlying structure of the data itself. |
| **Example** | Classifying an image as a Cat or Dog. | Generating an image of a Cat. |

---

## 3. Learning Generative Models

How do we find the parameters $\theta$ that make $p_{\theta}(x)$ similar to $p_{data}(x)$?

### Challenge 1: The Unknown Target
We do not know the true distribution $p_{data}(x)$. We only have samples.
Therefore, we define the **Empirical Distribution** $\hat{p}_{data}(x)$ based on the dataset:
$$ \hat{p}_{data}(x) = \frac{1}{N} \sum_{i=1}^{N} \delta(x - x_i) $$
*Where $\delta$ is the Dirac delta function.*

### Challenge 2: The Metric (Distance)
We need a way to measure the "distance" between two distributions. The standard metric is the **Kullback-Leibler (KL) Divergence**:

$$ D_{KL}[p(x) || q(x)] = \mathbb{E}_{p(x)} \left[ \log \frac{p(x)}{q(x)} \right] $$

*   $D_{KL} \ge 0$ (Non-negative).
*   $D_{KL} = 0$ if and only if distributions are identical.
*   **Note:** It is not symmetric ($D_{KL}[p||q] \neq D_{KL}[q||p]$).

### The Learning Objective: MLE
We want to minimize the divergence between the empirical distribution and our model:
$$ \hat{\theta} = \underset{\theta}{\text{argmin}} \ D_{KL}[\hat{p}_{data}(x) || p_{\theta}(x)] $$

Expanding the KL term:
$$ D_{KL}[\hat{p}_{data} || p_{\theta}] = \mathbb{E}_{\hat{p}_{data}}[\log \hat{p}_{data}(x)] - \mathbb{E}_{\hat{p}_{data}}[\log p_{\theta}(x)] $$

The first term is constant regarding $\theta$. Minimizing the KL divergence is equivalent to **maximizing** the second term:
$$ \hat{\theta} = \underset{\theta}{\text{argmax}} \sum_{i=1}^{N} \log p_{\theta}(x_i) $$

This is exactly **Maximum Likelihood Estimation (MLE)**.
> **Conclusion:** Learning a generative model by minimizing KL divergence from the empirical distribution is mathematically equivalent to Maximum Likelihood Estimation.

### Important Note on Generalization
MLE minimizes **Empirical Risk** (distance to the training set). Ideally, we want to minimize **Generalization Risk** (distance to the true distribution). If the sample size $N$ is small, the empirical distribution is a poor approximation of the true distribution, leading to **overfitting**.

---

## 4. Examples of Learning

### Example 1: Coin Toss (Bernoulli Distribution)
*   **Data:** Binary scalars (Heads=1, Tails=0). Dataset $X = \{1, 0, 0\}$.
*   **Model:** Bernoulli distribution $p_{\mu}(x) = \mu^x (1-\mu)^{1-x}$.
*   **Parameter:** $\mu$ (probability of heads).
*   **Log-Likelihood:** $\sum [x_i \log \mu + (1-x_i)\log(1-\mu)]$.
*   **MLE Result:** Differentiating with respect to $\mu$ and setting to 0 gives:
    $$ \hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i $$
    *(The sample mean).*

### Example 2: MNIST Images (Multivariate Bernoulli)
*   **Data:** $28 \times 28$ pixel images, binarized to $\{0, 1\}$. Treated as a vector $x \in \{0, 1\}^D$.
*   **Assumption:** Pixels are independent (Naive Bayes assumption).
*   **Model:** Product of independent Bernoulli distributions for each pixel.
    $$ p_{\mu}(\mathbf{x}) = \prod_{d=1}^{D} \mu_d^{x_d} (1-\mu_d)^{1-x_d} $$
*   **MLE Result:** The parameter for each pixel is simply the average intensity of that pixel across the dataset.
*   **Result Visualization:** The generated image looks like a blurry "average" of all digits overlaid.

### Example 3: Continuous Data (Multivariate Gaussian)
*   **Data:** Real-valued vectors $\mathbf{x} \in \mathbb{R}^D$ (e.g., clustered points).
*   **Model:** Multivariate Gaussian (Normal) Distribution $\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})$.
*   **Parameters:** Mean vector $\boldsymbol{\mu}$ and Covariance matrix $\boldsymbol{\Sigma}$.
*   **MLE Result:**
    *   $\hat{\boldsymbol{\mu}} = \frac{1}{N} \sum \mathbf{x}_i$ (Sample Mean)
    *   $\hat{\boldsymbol{\Sigma}} = \frac{1}{N} \sum (\mathbf{x}_i - \hat{\boldsymbol{\mu}})(\mathbf{x}_i - \hat{\boldsymbol{\mu}})^T$ (Sample Covariance)

---

## 5. Introduction to Latent Variables

### The Limitation of Simple Models
In the examples above, we modeled the observed variables directly.
*   **Coin:** Assumed one coin. What if there are multiple coins with different biases?
*   **MNIST:** Assumed one cluster. But MNIST has 10 distinct digits (0-9).
*   **Gaussian:** Assumed a single mode (peak). Real data is often multi-modal.

### Latent Variables ($z$)
To solve this, we assume there are hidden factors, or **Latent Variables ($z$)**, that govern the generation process but are not observed.
*   $z$ could represent "Which coin was chosen?" or "Which digit is being written?"

**Latent Variable Model (LVM):**
*   **Latent Variable $z$:** The hidden cause.
*   **Observed Variable $x$:** The data generated based on $z$.

**Graphical Model:**
$$ z \rightarrow x $$
*   $z \sim p(z)$ (Prior)
*   $x \sim p_{\theta}(x|z)$ (Likelihood)

*Lecture 2 will cover how to learn models with latent variables (Mixture Models, EM Algorithm, etc.).*

---

## 6. Appendix: MAP Estimation

**Maximum A Posteriori (MAP)** estimation is used to prevent overfitting (e.g., if you flip a coin 3 times and get 3 tails, MLE says probability of heads is 0).

*   **Concept:** Treat parameters $\theta$ as random variables with a **Prior Distribution** $p(\theta)$.
*   **Objective:** Maximize the posterior:
    $$ \hat{\theta}_{MAP} = \underset{\theta}{\text{argmax}} \left( \log p(X|\theta) + \log p(\theta) \right) $$
*   **Effect:** The prior acts as a **regularization term**.
*   **Example:** For the coin toss, using a Beta distribution as the prior adds "pseudo-counts" ($\alpha, \beta$) to the calculation:
    $$ \hat{\mu}_{MAP} = \frac{\sum x_i + \alpha - 1}{N + \alpha + \beta - 2} $$
    If $\alpha=\beta=2$, it's like having seen one head and one tail before starting, preventing extreme estimates (0 or 1) on small data.
